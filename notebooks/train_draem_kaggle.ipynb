{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAEM Training on Kaggle - Stage 2: Defect Localization\n",
    "\n",
    "This notebook trains the DRAEM model for bike defect localization.\n",
    "\n",
    "**No annotation required!** Trains only on intact images.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "### 1. Upload Your Data\n",
    "\n",
    "Upload your data folder to Kaggle with this structure:\n",
    "```\n",
    "/kaggle/input/bike-data/\n",
    "‚îî‚îÄ‚îÄ intact/\n",
    "    ‚îú‚îÄ‚îÄ bike001.jpg\n",
    "    ‚îú‚îÄ‚îÄ bike002.jpg\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "### 2. Run All Cells\n",
    "\n",
    "Just run all cells in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation (FIXED)\n",
    "!pip uninstall -y numpy -q\n",
    "!pip install -q numpy==1.24.4\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q scikit-image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "CONFIG = {\n",
    "    'intact_dir': '/kaggle/input/datasetprivet/data/processed/intact',\n",
    "    'epochs': 150,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 0.0001,\n",
    "    'image_size': 256,\n",
    "    'train_split': 0.85,\n",
    "    'val_split': 0.15,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': True,\n",
    "    'min_perlin_scale': 0,\n",
    "    'max_perlin_scale': 6,\n",
    "    'augmentation_prob': 0.8,\n",
    "    'output_dir': '/kaggle/working',\n",
    "    'save_interval': 10,\n",
    "    'early_stopping_patience': 20,\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "print(\"‚úÖ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "intact_dir = Path(CONFIG['intact_dir'])\n",
    "\n",
    "if not intact_dir.exists():\n",
    "    print(f\"‚ùå ERROR: Data directory not found: {intact_dir}\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Upload your data to Kaggle\")\n",
    "    print(\"2. Update CONFIG['intact_dir'] in the cell above\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "else:\n",
    "    # Count images\n",
    "    image_files = []\n",
    "    for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "        image_files.extend(list(intact_dir.glob(f'**/{ext}')))\n",
    "    \n",
    "    print(f\"‚úÖ Data directory found: {intact_dir}\")\n",
    "    print(f\"‚úÖ Found {len(image_files)} intact images\")\n",
    "    \n",
    "    # Show sample images\n",
    "    if len(image_files) > 0:\n",
    "        fig, axes = plt.subplots(1, min(5, len(image_files)), figsize=(15, 3))\n",
    "        if len(image_files) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, img_path in enumerate(image_files[:5]):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'{img_path.name}\\n{img.size}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ready to train!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No images found! Please check your data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Perlin Noise Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Check Data\n",
    "intact_dir = Path(CONFIG['intact_dir'])\n",
    "\n",
    "image_files = []\n",
    "for ext in ['*.jpg', '*.png', '*.jpeg', '*.JPG', '*.PNG']:\n",
    "    image_files.extend(list(intact_dir.glob(ext)))\n",
    "\n",
    "print(f\"‚úÖ Found {len(image_files)} intact images\")\n",
    "print(f\"   Train: {int(len(image_files) * CONFIG['train_split'])}\")\n",
    "print(f\"   Val: {len(image_files) - int(len(image_files) * CONFIG['train_split'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Anomaly Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Perlin Noise (FIXED)\n",
    "def lerp(a, b, t):\n",
    "    return a + t * (b - a)\n",
    "\n",
    "def rand_perlin_2d(shape, res, fade=lambda t: 6*t**5 - 15*t**4 + 10*t**3):\n",
    "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
    "    d = (shape[0] // res[0], shape[1] // res[1])\n",
    "    grid = np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1\n",
    "    angles = 2 * np.pi * np.random.rand(res[0] + 1, res[1] + 1)\n",
    "    gradients = np.stack((np.cos(angles), np.sin(angles)), axis=-1)\n",
    "    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat(d[0], 0).repeat(d[1], 1)\n",
    "    dot = lambda grad, shift: (np.stack((grid[:shape[0], :shape[1], 0] + shift[0], grid[:shape[0], :shape[1], 1] + shift[1]), axis=-1) * grad[:shape[0], :shape[1]]).sum(axis=-1)\n",
    "    n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0])\n",
    "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
    "    n01 = dot(tile_grads([0, -1], [1, None]), [0, -1])\n",
    "    n11 = dot(tile_grads([1, None], [1, None]), [-1, -1])\n",
    "    t = fade(grid[:shape[0], :shape[1]])\n",
    "    return np.sqrt(2) * lerp(lerp(n00, n10, t[..., 0]), lerp(n01, n11, t[..., 0]), t[..., 1])\n",
    "\n",
    "def generate_perlin_noise_mask(shape, min_scale=0, max_scale=6):\n",
    "    perlin_scalex = 2 ** np.random.randint(min_scale, max_scale)\n",
    "    perlin_scaley = 2 ** np.random.randint(min_scale, max_scale)\n",
    "    perlin_noise = rand_perlin_2d((shape[0], shape[1]), (perlin_scalex, perlin_scaley))\n",
    "    perlin_noise = (perlin_noise - perlin_noise.min()) / (perlin_noise.max() - perlin_noise.min())\n",
    "    threshold = np.random.uniform(0.3, 0.7)\n",
    "    mask = (perlin_noise > threshold).astype(np.float32)\n",
    "    return mask, perlin_noise\n",
    "\n",
    "print(\"‚úÖ Perlin noise generator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. DRAEM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DRAEM Model Architecture\n",
    "# ============================================================================\n",
    "\n",
    "class EncoderReconstructive(nn.Module):\n",
    "    def __init__(self, in_channels, base_width):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_width, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(base_width, base_width, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.mp1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_width, base_width * 2, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(base_width * 2, base_width * 2, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.mp2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_width * 2, base_width * 4, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(base_width * 4, base_width * 4, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.mp3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_width * 4, base_width * 8, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(base_width * 8, base_width * 8, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.mp4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(base_width * 8, base_width * 8, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(base_width * 8, base_width * 8, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b1 = self.block1(x)\n",
    "        mp1 = self.mp1(b1)\n",
    "        b2 = self.block2(mp1)\n",
    "        mp2 = self.mp2(b2)\n",
    "        b3 = self.block3(mp2)\n",
    "        mp3 = self.mp3(b3)\n",
    "        b4 = self.block4(mp3)\n",
    "        mp4 = self.mp4(b4)\n",
    "        b5 = self.block5(mp4)\n",
    "        return b5\n",
    "\n",
    "\n",
    "class DecoderReconstructive(nn.Module):\n",
    "    def __init__(self, base_width, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(base_width * 8, base_width * 8, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(base_width * 8, base_width * 4, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(base_width * 4, base_width * 2, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width * 2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(base_width * 2, base_width, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_width),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Conv2d(base_width, out_channels, 1)\n",
    "    \n",
    "    def forward(self, b5):\n",
    "        up1 = self.up1(b5)\n",
    "        up2 = self.up2(up1)\n",
    "        up3 = self.up3(up2)\n",
    "        up4 = self.up4(up3)\n",
    "        output = self.final(up4)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ReconstructiveSubNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_width=128):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderReconstructive(in_channels, base_width)\n",
    "        self.decoder = DecoderReconstructive(base_width, out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b5 = self.encoder(x)\n",
    "        output = self.decoder(b5)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Similar implementation for Discriminative network...\n",
    "# (Code continues in next cell due to length)\n",
    "\n",
    "print(\"‚úÖ Reconstructive network defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset (FIXED)\n",
    "class DRAEMDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_size=256, augmentation_prob=0.8,\n",
    "                 min_perlin_scale=0, max_perlin_scale=6, is_train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_size = image_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.min_perlin_scale = min_perlin_scale\n",
    "        self.max_perlin_scale = max_perlin_scale\n",
    "        self.is_train = is_train\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def augment_with_transforms(self, image):\n",
    "        if not self.is_train:\n",
    "            return image\n",
    "        image = np.array(image, dtype=np.uint8, copy=True)\n",
    "        if np.random.rand() < 0.5:\n",
    "            image = np.fliplr(image).copy()\n",
    "        if np.random.rand() < 0.3:\n",
    "            image = np.flipud(image).copy()\n",
    "        if np.random.rand() < 0.5:\n",
    "            angle = np.random.uniform(-20, 20)\n",
    "            h, w = image.shape[:2]\n",
    "            M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "            image = cv2.warpAffine(np.ascontiguousarray(image, dtype=np.uint8), M, (w, h))\n",
    "        if np.random.rand() < 0.5:\n",
    "            image = cv2.convertScaleAbs(image, alpha=np.random.uniform(0.7, 1.3), beta=np.random.uniform(-30, 30))\n",
    "        return image\n",
    "    \n",
    "    def augment_image(self, image):\n",
    "        image = np.array(image, dtype=np.uint8, copy=True)\n",
    "        perlin_mask, _ = generate_perlin_noise_mask(image.shape[:2], self.min_perlin_scale, self.max_perlin_scale)\n",
    "        perlin_mask_3ch = np.repeat(perlin_mask[:, :, np.newaxis], 3, axis=2)\n",
    "        if np.random.rand() < 0.5:\n",
    "            anomaly_texture = np.random.randint(0, 255, image.shape, dtype=np.uint8)\n",
    "        else:\n",
    "            anomaly_texture = np.clip(image.astype(np.float32) * np.random.uniform(0.3, 1.5), 0, 255).astype(np.uint8)\n",
    "        augmented_image = (image.astype(np.float32) * (1 - perlin_mask_3ch) + anomaly_texture.astype(np.float32) * perlin_mask_3ch).astype(np.uint8)\n",
    "        return augmented_image, perlin_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        image = np.array(image, dtype=np.uint8, copy=True)\n",
    "        image = self.augment_with_transforms(image)\n",
    "        if np.random.rand() < self.augmentation_prob:\n",
    "            augmented_image, mask = self.augment_image(image)\n",
    "        else:\n",
    "            augmented_image = image.copy()\n",
    "            mask = np.zeros((self.image_size, self.image_size), dtype=np.float32)\n",
    "        image = np.array(image, dtype=np.uint8, copy=True)\n",
    "        augmented_image = np.array(augmented_image, dtype=np.uint8, copy=True)\n",
    "        mask = np.array(mask, dtype=np.float32, copy=True)\n",
    "        image_pil = Image.fromarray(image.copy())\n",
    "        augmented_pil = Image.fromarray(augmented_image.copy())\n",
    "        image_tensor = self.transform(image_pil)\n",
    "        augmented_tensor = self.transform(augmented_pil)\n",
    "        mask_tensor = torch.from_numpy(mask.copy()).unsqueeze(0)\n",
    "        return image_tensor, augmented_tensor, mask_tensor\n",
    "\n",
    "# Create datasets\n",
    "image_paths = []\n",
    "for ext in ['*.jpg', '*.png', '*.jpeg', '*.JPG', '*.PNG']:\n",
    "    image_paths.extend(list(Path(CONFIG['intact_dir']).glob(ext)))\n",
    "\n",
    "random.shuffle(image_paths)\n",
    "n_train = int(len(image_paths) * CONFIG['train_split'])\n",
    "train_paths = image_paths[:n_train]\n",
    "val_paths = image_paths[n_train:]\n",
    "\n",
    "train_dataset = DRAEMDataset(train_paths, CONFIG['image_size'], CONFIG['augmentation_prob'], \n",
    "                              CONFIG['min_perlin_scale'], CONFIG['max_perlin_scale'], True)\n",
    "val_dataset = DRAEMDataset(val_paths, CONFIG['image_size'], CONFIG['augmentation_prob'],\n",
    "                            CONFIG['min_perlin_scale'], CONFIG['max_perlin_scale'], False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                          num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Architecture\n",
    "class ReconstructiveSubNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, out_channels, 4, 2, 1), nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.bottleneck(self.encoder(x)))\n",
    "\n",
    "class DiscriminativeSubNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2, True),\n",
    "            nn.ConvTranspose2d(64, out_channels, 4, 2, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "class DRAEM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reconstructive = ReconstructiveSubNetwork()\n",
    "        self.discriminative = DiscriminativeSubNetwork()\n",
    "    def forward(self, x):\n",
    "        reconstruction = self.reconstructive(x)\n",
    "        segmentation = self.discriminative(torch.cat([x, reconstruction], dim=1))\n",
    "        return reconstruction, segmentation\n",
    "\n",
    "print(\"‚úÖ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: SSIM Loss\n",
    "def ssim(img1, img2, window_size=11):\n",
    "    img1 = (img1 + 1) / 2\n",
    "    img2 = (img2 + 1) / 2\n",
    "    channel = img1.size(1)\n",
    "    def gaussian(ws, sigma=1.5):\n",
    "        gauss = torch.Tensor([np.exp(-(x - ws//2)**2 / (2*sigma**2)) for x in range(ws)])\n",
    "        return gauss / gauss.sum()\n",
    "    _1D = gaussian(window_size).unsqueeze(1)\n",
    "    _2D = _1D.mm(_1D.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D.expand(channel, 1, window_size, window_size).contiguous().to(img1.device)\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "    mu1_sq, mu2_sq, mu1_mu2 = mu1.pow(2), mu2.pow(2), mu1 * mu2\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
    "    C1, C2 = 0.01 ** 2, 0.03 ** 2\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "print(\"‚úÖ SSIM ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Loss Functions\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "class DRAEMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l2_loss = nn.MSELoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "    def forward(self, reconstruction, segmentation, target_img, target_mask):\n",
    "        recon_loss = self.l2_loss(reconstruction, target_img)\n",
    "        ssim_loss = 1 - ssim(reconstruction, target_img)\n",
    "        seg_loss = self.focal_loss(segmentation, target_mask.squeeze(1).long())\n",
    "        return recon_loss + ssim_loss + seg_loss, recon_loss, ssim_loss, seg_loss\n",
    "\n",
    "print(\"‚úÖ Losses ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Functions\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = recon_loss = ssim_loss = seg_loss = 0\n",
    "    for intact, augmented, mask in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        intact, augmented, mask = intact.to(device), augmented.to(device), mask.to(device)\n",
    "        reconstruction, segmentation = model(augmented)\n",
    "        loss, r_loss, s_loss, sg_loss = criterion(reconstruction, segmentation, intact, mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        recon_loss += r_loss.item()\n",
    "        ssim_loss += s_loss.item()\n",
    "        seg_loss += sg_loss.item()\n",
    "    n = len(loader)\n",
    "    return {'total': total_loss/n, 'recon': recon_loss/n, 'ssim': ssim_loss/n, 'seg': seg_loss/n}\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = recon_loss = ssim_loss = seg_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for intact, augmented, mask in loader:\n",
    "            intact, augmented, mask = intact.to(device), augmented.to(device), mask.to(device)\n",
    "            reconstruction, segmentation = model(augmented)\n",
    "            loss, r_loss, s_loss, sg_loss = criterion(reconstruction, segmentation, intact, mask)\n",
    "            total_loss += loss.item()\n",
    "            recon_loss += r_loss.item()\n",
    "            ssim_loss += s_loss.item()\n",
    "            seg_loss += sg_loss.item()\n",
    "    n = len(loader)\n",
    "    return {'total': total_loss/n, 'recon': recon_loss/n, 'ssim': ssim_loss/n, 'seg': seg_loss/n}\n",
    "\n",
    "print(\"‚úÖ Training functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main Training Loop (FINAL FIXED VERSION)\n",
    "device = torch.device(CONFIG['device'])\n",
    "model = DRAEM().to(device)\n",
    "criterion = DRAEMLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "# FIXED: Remove 'verbose' parameter (doesn't exist in ReduceLROnPlateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Starting training for {CONFIG['epochs']} epochs...\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [], \n",
    "    'train_recon': [], 'val_recon': [],\n",
    "    'train_ssim': [], 'val_ssim': [], \n",
    "    'train_seg': [], 'val_seg': [], \n",
    "    'learning_rates': []\n",
    "}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    val_metrics = validate(model, val_loader, criterion, device)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store metrics\n",
    "    history['train_loss'].append(train_metrics['total'])\n",
    "    history['val_loss'].append(val_metrics['total'])\n",
    "    history['train_recon'].append(train_metrics['recon'])\n",
    "    history['val_recon'].append(val_metrics['recon'])\n",
    "    history['train_ssim'].append(train_metrics['ssim'])\n",
    "    history['val_ssim'].append(val_metrics['ssim'])\n",
    "    history['train_seg'].append(train_metrics['seg'])\n",
    "    history['val_seg'].append(val_metrics['seg'])\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TRAIN - Total: {train_metrics['total']:.4f}, Recon: {train_metrics['recon']:.4f}, \"\n",
    "          f\"SSIM: {train_metrics['ssim']:.4f}, Seg: {train_metrics['seg']:.4f}\")\n",
    "    print(f\"VAL   - Total: {val_metrics['total']:.4f}, Recon: {val_metrics['recon']:.4f}, \"\n",
    "          f\"SSIM: {val_metrics['ssim']:.4f}, Seg: {val_metrics['seg']:.4f}\")\n",
    "    print(f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    old_lr = current_lr\n",
    "    scheduler.step(val_metrics['total'])\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"üìâ Learning rate reduced: {old_lr:.6f} ‚Üí {new_lr:.6f}\")\n",
    "    \n",
    "    if val_metrics['total'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['total']\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "            'config': CONFIG,\n",
    "            'history': history\n",
    "        }, f\"{CONFIG['output_dir']}/draem_best.pth\")\n",
    "        print(f\"‚≠ê NEW BEST MODEL! Val Loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epochs\")\n",
    "    \n",
    "    if (epoch + 1) % CONFIG['save_interval'] == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_metrics['total'],\n",
    "            'config': CONFIG\n",
    "        }, f\"{CONFIG['output_dir']}/draem_epoch_{epoch+1}.pth\")\n",
    "        print(f\"üíæ Checkpoint saved: epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚ö†Ô∏è EARLY STOPPING at epoch {epoch+1}\")\n",
    "        print(f\"No improvement for {CONFIG['early_stopping_patience']} epochs\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Total Epochs: {len(history['train_loss'])}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Download Model\n",
    "\n",
    "Download the trained model from Kaggle to use locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save Final Model\n",
    "torch.save({'model_state_dict': model.state_dict(), 'config': CONFIG, \n",
    "           'history': history, 'best_val_loss': best_val_loss},\n",
    "          f\"{CONFIG['output_dir']}/draem_final.pth\")\n",
    "print(\"‚úÖ Final model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Plot Training History\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs = range(1, len(history['train_loss'])+1)\n",
    "\n",
    "axes[0,0].plot(epochs, history['train_loss'], 'b-', label='Train', lw=2)\n",
    "axes[0,0].plot(epochs, history['val_loss'], 'r-', label='Val', lw=2)\n",
    "axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].plot(epochs, history['train_recon'], 'b-', label='Train', lw=2)\n",
    "axes[0,1].plot(epochs, history['val_recon'], 'r-', label='Val', lw=2)\n",
    "axes[0,1].set_title('Reconstruction Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,0].plot(epochs, history['train_ssim'], 'b-', label='Train', lw=2)\n",
    "axes[1,0].plot(epochs, history['val_ssim'], 'r-', label='Val', lw=2)\n",
    "axes[1,0].set_title('SSIM Loss')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].plot(epochs, history['train_seg'], 'b-', label='Train', lw=2)\n",
    "axes[1,1].plot(epochs, history['val_seg'], 'r-', label='Val', lw=2)\n",
    "axes[1,1].set_title('Segmentation Loss')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['output_dir']}/training_history.png\", dpi=300)\n",
    "plt.show()\n",
    "print(\"‚úÖ Training history saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Total Epochs: {len(history['train_loss'])}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in Path(CONFIG['output_dir']).glob('*.pth'):\n",
    "    print(f\"  ‚Ä¢ {f.name} ({f.stat().st_size/1e6:.2f} MB)\")\n",
    "print(\"\\nüéâ SUCCESS! Download draem_best.pth for deployment!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done! üéâ\n",
    "\n",
    "Your DRAEM model is trained!\n",
    "\n",
    "**Download `draem_final.pth` and use it for inference.**\n",
    "\n",
    "**No annotation was needed!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
